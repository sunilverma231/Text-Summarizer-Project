# üéØ Model Quality Improvement Guide - LoRA Configuration

## Problem Analysis

**Current Performance:**
```
ROUGE Scores: 0.018 (Very Low)
Training Data: 40 samples (Too small)
Training Epochs: 1 (Insufficient)
LoRA Rank: 8 (Limited capacity)
```

**Root Causes:**
- ‚úó Insufficient training data (40 vs ~1600 samples available)
- ‚úó Too few epochs (1 vs 3 needed)
- ‚úó Insufficient warmup (10 vs 100 steps)
- ‚úó Lower LoRA rank (8 vs 16 for better quality)

---

## Updated Configuration (Quality Mode)

### Training Settings

| Parameter | Old | New | Impact |
|-----------|-----|-----|--------|
| Training Samples | 40 | ~1600 (80% of dataset) | **9.6x more data** |
| Epochs | 1 | 3 | **3x better convergence** |
| LoRA Rank | 8 | 16 | **2x more parameters** |
| Warmup Steps | 10 | 100 | **Better stability** |
| Learning Rate | Auto | 5e-4 | **Controlled learning** |
| Eval Frequency | 20 steps | 50 steps | **Better monitoring** |

---

## ‚è±Ô∏è Training & Evaluation Time Estimates

### On macOS CPU (Your Current Setup)

**Training Phase:**
- Data Loading: ~5 minutes
- Per epoch: ~45-50 minutes
- Total training (3 epochs): **~2.5-3 hours**

**Evaluation Phase:**
- Model loading: ~2 minutes
- Test dataset evaluation: ~8-10 minutes
- Metrics calculation: ~2-3 minutes
- **Total evaluation: ~12-15 minutes**

**Complete Pipeline (Training + Evaluation):**
```
Total Time: ~2.5-3.5 hours (150-210 minutes)
```

### For GPU (If available)
- Training: ~20-30 minutes (5-10x faster)
- Evaluation: ~2-3 minutes
- Total: ~30-40 minutes

---

## Expected Results After Training

| Metric | Current | Expected | Target |
|--------|---------|----------|--------|
| ROUGE1 | 0.018 | 0.35-0.45 | >0.40 |
| ROUGE2 | 0.000 | 0.15-0.25 | >0.20 |
| ROUGEL | 0.018 | 0.30-0.40 | >0.35 |
| ROUGELsum | 0.018 | 0.30-0.40 | >0.35 |

---

## How to Run Improved Training

### Step 1: Stop any running processes
```bash
pkill -f "python"
pkill -f "app.py"
```

### Step 2: Start training with quality settings
```bash
conda activate textS
python main.py
```

### Step 3: Monitor progress
The script will show:
```
============================================================
TRAINING CONFIGURATION (LoRA - Quality Mode)
============================================================
Training samples: 1614
Evaluation samples: 380
Number of epochs: 3
Warmup steps: 100
Learning rate: 5e-4
LoRA rank: 16 (higher = better quality, slower training)

ESTIMATED TIME:
  Training: ~2-3 hours on CPU
  Evaluation: ~10-15 minutes
  Total: ~2.5-3.5 hours
============================================================
```

### Step 4: Check results
After training completes:
```bash
cat artifacts/model_evaluation/metrics.csv
```

---

## What's Different This Time?

### LoRA Rank Improvement
**Old:** `r=8` (limited capacity)
```
- ~66K trainable parameters
- Faster training
- Lower quality
```

**New:** `r=16` (better capacity)
```
- ~130K trainable parameters
- Balanced speed/quality
- Significantly better results
```

### Data Quality
**Old:** 40 samples from test set
```
- Incomplete representation
- Poor generalization
```

**New:** ~1600 samples (80% of train set)
```
- Better representation
- Proper generalization
- Standard ML practice
```

### Training Process
**Old:** 1 epoch with minimal warmup
```
- No convergence
- Unstable learning
```

**New:** 3 epochs with proper warmup
```
- Multiple passes through data
- Better convergence
- More stable learning
```

---

## Performance Comparison Timeline

```
Training Progress (Expected):
‚îú‚îÄ Epoch 1: ROUGE1 improves from 0.02 ‚Üí 0.25
‚îú‚îÄ Epoch 2: ROUGE1 improves from 0.25 ‚Üí 0.40
‚îî‚îÄ Epoch 3: ROUGE1 stabilizes at 0.40-0.45

Evaluation:
‚îî‚îÄ Final ROUGE Scores: ~0.35-0.45 (Good!)
```

---

## If Training Takes Too Long

**Option 1: Balance Quality & Speed**
Edit `model_trainer.py` and change:
```python
"num_train_epochs": 2,  # Instead of 3
train_size = int(len(dataset_samsum_pt["train"]) * 0.5)  # Instead of 0.8 (use 50%)
```
**Estimated time: ~1.5-2 hours**

**Option 2: Fast Training Mode**
```python
"num_train_epochs": 1,
train_size = int(len(dataset_samsum_pt["train"]) * 0.3)  # 30% of data
"num_train_epochs": 1,
```
**Estimated time: ~30-45 minutes**
**Quality: Good but not optimal**

---

## Monitoring During Training

The training will output:
```
Step 100/4842 | Loss: 3.45 | Learning Rate: 5e-4
Step 200/4842 | Loss: 2.87 | Learning Rate: 5e-4
...
Epoch 1/3 completed | Eval Loss: 2.34
```

Good signs:
- ‚úÖ Loss decreasing over time
- ‚úÖ Eval loss lower than train loss
- ‚úÖ Regular checkpoint saves

Bad signs:
- ‚ùå Loss not decreasing
- ‚ùå NaN (Not a Number) values
- ‚ùå Out of memory errors

---

## After Training

### Test the Model
```bash
python app.py
```
Then test at: http://localhost:8000/docs

### Check Metrics
```bash
cat artifacts/model_evaluation/metrics.csv
```

### Expected metrics.csv
```csv
rouge1,rouge2,rougeL,rougeLsum
0.42,0.21,0.38,0.39
```

---

## Troubleshooting

### Out of Memory
If you get memory errors:
```python
"per_device_train_batch_size": 1,  # Already 1, can't reduce
"gradient_accumulation_steps": 2,  # Reduce from 4
```

### Training too slow
```python
"num_train_epochs": 2,  # Reduce from 3
"eval_steps": 100,  # Evaluate less frequently
```

### Disk space issues
```bash
rm -rf artifacts/model_trainer/checkpoint-*
# Keep only the final model
```

---

## Summary

| Item | Details |
|------|---------|
| **Training Data** | 1614 samples (80% of dataset) |
| **Training Time** | ~2.5-3 hours on CPU |
| **Evaluation Time** | ~12-15 minutes |
| **Expected ROUGE1** | 0.40-0.45 |
| **Model Quality** | Good ‚Üë (from Very Low) |
| **LoRA Rank** | 16 (Better capacity) |
| **Output File** | artifacts/model_evaluation/metrics.csv |

---

**Start training with:**
```bash
python main.py
```

Monitor progress and wait for completion! ‚òï

*Estimated completion time: ~3 hours from start*
