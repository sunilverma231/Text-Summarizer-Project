{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d291820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force CPU mode - MUST run this cell FIRST before any other cells\n",
    "import os\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '0'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "print(\"\u2713 Forcing CPU mode (MPS disabled)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a924ad61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c13ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282cb4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcb9bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5108526d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainerConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    model_ckpt: str\n",
    "    num_train_epochs: int\n",
    "    warmup_steps: int\n",
    "    per_device_train_batch_size: int\n",
    "    weight_decay: float\n",
    "    logging_steps: int\n",
    "    evaluation_strategy: str\n",
    "    eval_steps: int\n",
    "    save_steps: int\n",
    "    gradient_accumulation_steps: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ca5fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_summarizer.constants import *\n",
    "from text_summarizer.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3877220",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from text_summarizer.constants import CONFIG_FILE_PATH, PARAMS_FILE_PATH\n",
    "from text_summarizer.utils.common import read_yaml, create_directories\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath=CONFIG_FILE_PATH,\n",
    "        params_filepath=PARAMS_FILE_PATH,\n",
    "    ):\n",
    "        # Convert relative paths to absolute\n",
    "        if not config_filepath.is_absolute():\n",
    "            config_filepath = Path(os.getcwd()) / config_filepath\n",
    "        if not params_filepath.is_absolute():\n",
    "            params_filepath = Path(os.getcwd()) / params_filepath\n",
    "        \n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_model_trainer_config(self) -> ModelTrainerConfig:\n",
    "        model_trainer_config = self.config.model_trainer\n",
    "        training_args = self.params.TrainingArguments\n",
    "\n",
    "        model_trainer_config = ModelTrainerConfig(\n",
    "            root_dir=Path(model_trainer_config.root_dir),\n",
    "            data_path=Path(model_trainer_config.data_path),\n",
    "            model_ckpt=Path(model_trainer_config.model_ckpt),\n",
    "            num_train_epochs=training_args.num_train_epochs,\n",
    "            warmup_steps=training_args.warmup_steps,\n",
    "            per_device_train_batch_size=training_args.per_device_train_batch_size,\n",
    "            weight_decay=training_args.weight_decay,\n",
    "            logging_steps=training_args.logging_steps,\n",
    "            evaluation_strategy=training_args.evaluation_strategy,\n",
    "            eval_steps=training_args.eval_steps,\n",
    "            save_steps=training_args.save_steps,\n",
    "            gradient_accumulation_steps=training_args.gradient_accumulation_steps,\n",
    "        )\n",
    "        return model_trainer_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc371668",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer \n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from datasets import load_dataset, load_from_disk \n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cde2c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# trainer_args = TrainingArguments(\n",
    "#     output_dir='pegasus-samsum', num_train_epochs=1, warmup_steps=500,\n",
    "#     per_device_train_batch_size=1, per_device_eval_batch_size=1,\n",
    "#     weight_decay=0.01, logging_steps=10,\n",
    "#     eval_strategy='steps', eval_steps=500, save_steps=1e6,\n",
    "#     gradient_accumulation_steps=16\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38957065",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer: \n",
    "    def __init__(self, config: ModelTrainerConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def train(self):\n",
    "        \n",
    "        # Force CPU device explicitly\n",
    "        device = \"cpu\"\n",
    "        print(f\"Using device: {device}\")\n",
    "\n",
    "        # Ensure Transformers logs show up and tqdm is enabled\n",
    "        try:\n",
    "            from transformers.utils import logging as hf_logging\n",
    "            hf_logging.set_verbosity_info()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.config.model_ckpt)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(self.config.model_ckpt, use_safetensors=True)\n",
    "        \n",
    "        # Explicitly move model to CPU\n",
    "        model = model.to(device)\n",
    "        \n",
    "        seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "        data_collator = seq2seq_data_collator\n",
    "\n",
    "        # Loading the dataset from the data path\n",
    "        dataset_samsum_pt = load_from_disk(self.config.data_path)\n",
    "        \n",
    "        # Use 40 samples - calibrated for ~1 hour training on CPU\n",
    "        train_dataset = dataset_samsum_pt['test'].select(range(min(40, len(dataset_samsum_pt['test']))))\n",
    "        eval_dataset = dataset_samsum_pt['validation'].select(range(min(10, len(dataset_samsum_pt['validation']))))\n",
    "        print(f\"Training on {len(train_dataset)} samples\")\n",
    "        print(f\"Evaluating on {len(eval_dataset)} samples\")\n",
    "\n",
    "        # Training configuration - optimized for ~1 hour completion\n",
    "        trainer_args = TrainingArguments(\n",
    "            output_dir=self.config.root_dir,\n",
    "            num_train_epochs=1,\n",
    "            warmup_steps=10,\n",
    "            per_device_train_batch_size=1,\n",
    "            weight_decay=0.01,\n",
    "            logging_strategy='steps',\n",
    "            logging_steps=5,\n",
    "            disable_tqdm=False,\n",
    "            report_to='none',\n",
    "            eval_strategy='steps',\n",
    "            eval_steps=20,\n",
    "            save_steps=int(1e6),\n",
    "            gradient_accumulation_steps=4,\n",
    "            no_cuda=True,\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=trainer_args,\n",
    "            tokenizer=tokenizer,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            data_collator=data_collator,\n",
    "        )\n",
    "\n",
    "        print(\"Starting training...\")\n",
    "        train_output = trainer.train()\n",
    "        \n",
    "        try:\n",
    "            steps = int(train_output.global_step)\n",
    "            print(f\"Training finished. Total steps: {steps}\")\n",
    "        except Exception:\n",
    "            pass\n",
    "        print(\"Training completed!\")\n",
    "\n",
    "        # Saving the model after training\n",
    "        model.save_pretrained(os.path.join(self.config.root_dir, \"pegasus-samsum-model\"))\n",
    "        tokenizer.save_pretrained(os.path.join(self.config.root_dir, \"tokenizer\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd70da8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_trainer_config = config.get_model_trainer_config()\n",
    "    model_trainer_config = ModelTrainer(config=model_trainer_config)\n",
    "    model_trainer_config.train()\n",
    "except Exception as e: \n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47234ced",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ff65b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}