# End-to-End Text Summarizer with LoRA Fine-Tuning

A production-ready text summarization system built with **FastAPI**, **Hugging Face Transformers**, and **Parameter-Efficient Fine-Tuning (PEFT/LoRA)**. This project demonstrates a complete MLOps pipeline from data ingestion to deployment on AWS ECS.

## ğŸš€ Features

- **LoRA-based Fine-Tuning**: Migrated from full PEGASUS fine-tuning to Parameter-Efficient Fine-Tuning (LoRA) for faster training and lower resource consumption
- **FastAPI REST API**: Production-ready endpoints for training and inference
- **Modular Pipeline**: Separate stages for data ingestion, validation, transformation, training, and evaluation
- **Docker & AWS ECS Ready**: Optimized Dockerfile with Gunicorn/Uvicorn workers for cloud deployment
- **Automatic Fallback**: Smart model loading with fallback to base HuggingFace models if local artifacts are missing
- **Comprehensive Logging**: Full pipeline logging for debugging and monitoring

## ï¿½ï¸ Tech Stack

### Core ML/NLP Frameworks
- **ğŸ¤— Transformers** - Pre-trained models and tokenizers (PEGASUS)
- **PEFT (Parameter-Efficient Fine-Tuning)** - LoRA adapters for efficient training
- **Datasets** - Hugging Face dataset loading and processing
- **Evaluate** - ROUGE score computation
- **PyTorch** - Deep learning backend

### Web Framework & API
- **FastAPI** - Modern, fast web framework for building APIs
- **Uvicorn** - Lightning-fast ASGI server
- **Gunicorn** - Production WSGI server with worker management
- **Jinja2** - Template engine for web pages

### Performance Optimization
- **uvloop** - Ultra-fast asyncio event loop (Linux/macOS)
- **httptools** - High-performance HTTP request parsing
- **orjson** - Fast JSON serialization/deserialization

### Data Processing
- **Pandas** - Data manipulation and analysis
- **NLTK** - Natural language processing utilities
- **sacrebleu** - BLEU score calculation
- **rouge_score** - ROUGE metric implementation

### Deployment & DevOps
- **Docker** - Containerization
- **AWS ECS (Fargate)** - Container orchestration
- **AWS ECR** - Container registry
- **boto3** - AWS SDK for Python

### Development Tools
- **Conda** - Environment and package management
- **Jupyter Notebook** - Interactive development and experimentation
- **PyYAML** - YAML configuration parsing
- **python-box** - Dict-like object access
- **tqdm** - Progress bars

### Utilities
- **py7zr** - 7z archive extraction
- **ensure** - Assertion library
- **mypy-boto3-s3** - Type hints for boto3 S3

## ï¿½ğŸ“Š Project Structure

```
Text-Summarizer-Project/
â”‚
â”œâ”€â”€ app.py                          # FastAPI application with /predict and /train endpoints
â”œâ”€â”€ main.py                         # Pipeline orchestrator (runs all stages)
â”œâ”€â”€ Dockerfile                      # Production-ready Docker image
â”œâ”€â”€ requirements.txt                # Python dependencies
â”œâ”€â”€ setup.py                        # Package installation
â”œâ”€â”€ params.yaml                     # Training hyperparameters
â”œâ”€â”€ README.md                       # This file
â”œâ”€â”€ DEPLOY_ECS.md                   # AWS ECS deployment guide
â”œâ”€â”€ FAST_TRAINING.md                # Fast training mode documentation
â”œâ”€â”€ LORA_MIGRATION.md               # LoRA migration details
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml                 # Project configuration (paths, model names)
â”‚
â”œâ”€â”€ src/text_summarizer/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ constants/                  # Constants and paths
â”‚   â”œâ”€â”€ entity/                     # Data classes for configs
â”‚   â”œâ”€â”€ logging/                    # Custom logging setup
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ common.py               # Helper functions (read YAML, create dirs)
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ configuration.py        # Configuration manager
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ data_ingestion.py       # Download and extract dataset
â”‚   â”‚   â”œâ”€â”€ data_validation.py      # Validate dataset schema
â”‚   â”‚   â”œâ”€â”€ data_transformation.py  # Tokenize and prepare data
â”‚   â”‚   â”œâ”€â”€ model_trainer.py        # LoRA fine-tuning with PEFT
â”‚   â”‚   â””â”€â”€ model_evaluation.py     # ROUGE score evaluation
â”‚   â””â”€â”€ pipeline/
â”‚       â”œâ”€â”€ stage_01_data_ingestion.py
â”‚       â”œâ”€â”€ stage_02_data_validation.py
â”‚       â”œâ”€â”€ stage_03_data_transformation.py
â”‚       â”œâ”€â”€ stage_04_model_trainer.py
â”‚       â”œâ”€â”€ stage_05_model_evaluation.py
â”‚       â””â”€â”€ prediction.py           # Prediction pipeline with LoRA support
â”‚
â”œâ”€â”€ research/                       # Jupyter notebooks for experimentation
â”‚   â”œâ”€â”€ 01_data_ingestion.ipynb
â”‚   â”œâ”€â”€ 02_data_validation.ipynb
â”‚   â”œâ”€â”€ 03_data_transformation.ipynb
â”‚   â”œâ”€â”€ 04_model_trainer.ipynb
â”‚   â””â”€â”€ 05_model_evaluation.ipynb
â”‚
â”œâ”€â”€ artifacts/                      # Generated during pipeline execution
â”‚   â”œâ”€â”€ data_ingestion/             # Downloaded SAMSum dataset
â”‚   â”œâ”€â”€ data_validation/            # Validation status
â”‚   â”œâ”€â”€ data_transformation/        # Tokenized datasets
â”‚   â”œâ”€â”€ model_trainer/              # LoRA adapters and tokenizer
â”‚   â””â”€â”€ model_evaluation/           # metrics.csv with ROUGE scores
â”‚
â””â”€â”€ tests/                          # Unit tests
    â””â”€â”€ unit/

```


# Project Title

A brief description of what this project does and who it's for
# ğŸš€ End-to-End Text Summarizer with LoRA Fine-Tuning

[![CI/CD Pipeline - Deploy Docker on EC2](https://github.com/sunilverma231/Text-Summarizer-Project/actions/workflows/main.yaml/badge.svg)](https://github.com/sunilverma231/Text-Summarizer-Project/actions/workflows/main.yaml)
[![Live Demo](https://img.shields.io/badge/Live-Demo-brightgreen)](http://51.20.86.12:8000)
![AWS](https://img.shields.io/badge/AWS-EC2-orange)
![Docker](https://img.shields.io/badge/Docker-Containerized-blue)
![FastAPI](https://img.shields.io/badge/FastAPI-Production--ready-teal)
![Python](https://img.shields.io/badge/Python-3.10-blue)

ğŸ”— **Live Application**: http://51.20.86.12:8000  
ğŸ“˜ **Swagger Docs**: http://51.20.86.12:8000/docs  

---

## ğŸ“Œ Overview

A **production-ready text summarization system** built using **FastAPI**, **Hugging Face Transformers**, and **Parameter-Efficient Fine-Tuning (LoRA / PEFT)**.

This project demonstrates a **full MLOps lifecycle**:
- Data ingestion â†’ training â†’ evaluation
- Model serving via FastAPI
- Dockerized deployment
- CI/CD using GitHub Actions
- **Automated deployment on AWS EC2 using a self-hosted runner**

---

## ğŸš€ Features

- âœ… LoRA-based fine-tuning (PEFT)
- âœ… FastAPI REST API (`/train`, `/predict`)
- âœ… Modular ML pipeline architecture
- âœ… Docker + Gunicorn + Uvicorn
- âœ… CI/CD pipeline with GitHub Actions
- âœ… AWS ECR + EC2 deployment
- âœ… Self-hosted GitHub Actions runner
- âœ… Graceful model fallback
- âœ… Health checks & logging

---

## ğŸ§  Tech Stack

### Machine Learning / NLP
- ğŸ¤— Transformers (PEGASUS)
- PEFT (LoRA)
- PyTorch
- ROUGE evaluation
- Hugging Face Datasets

### Backend
- FastAPI
- Gunicorn + Uvicorn
- Jinja2

### DevOps & Cloud
- Docker
- AWS EC2
- AWS ECR
- GitHub Actions (CI/CD)
- Self-hosted GitHub Runner

---

## ğŸ“‚ Project Structure

```text
Text-Summarizer-Project/
â”‚
â”œâ”€â”€ app.py                          # FastAPI application with /predict and /train endpoints
â”œâ”€â”€ main.py                         # Pipeline orchestrator (runs all stages)
â”œâ”€â”€ Dockerfile                      # Production-ready Docker image
â”œâ”€â”€ requirements.txt                # Python dependencies
â”œâ”€â”€ setup.py                        # Package installation
â”œâ”€â”€ params.yaml                     # Training hyperparameters
â”œâ”€â”€ README.md                       # This file
â”œâ”€â”€ DEPLOY_ECS.md                   # AWS ECS deployment guide
â”œâ”€â”€ FAST_TRAINING.md                # Fast training mode documentation
â”œâ”€â”€ LORA_MIGRATION.md               # LoRA migration details
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml                 # Project configuration (paths, model names)
â”‚
â”œâ”€â”€ src/text_summarizer/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ constants/                  # Constants and paths
â”‚   â”œâ”€â”€ entity/                     # Data classes for configs
â”‚   â”œâ”€â”€ logging/                    # Custom logging setup
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ common.py               # Helper functions (read YAML, create dirs)
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ configuration.py        # Configuration manager
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ data_ingestion.py       # Download and extract dataset
â”‚   â”‚   â”œâ”€â”€ data_validation.py      # Validate dataset schema
â”‚   â”‚   â”œâ”€â”€ data_transformation.py  # Tokenize and prepare data
â”‚   â”‚   â”œâ”€â”€ model_trainer.py        # LoRA fine-tuning with PEFT
â”‚   â”‚   â””â”€â”€ model_evaluation.py     # ROUGE score evaluation
â”‚   â””â”€â”€ pipeline/
â”‚       â”œâ”€â”€ stage_01_data_ingestion.py
â”‚       â”œâ”€â”€ stage_02_data_validation.py
â”‚       â”œâ”€â”€ stage_03_data_transformation.py
â”‚       â”œâ”€â”€ stage_04_model_trainer.py
â”‚       â”œâ”€â”€ stage_05_model_evaluation.py
â”‚       â””â”€â”€ prediction.py           # Prediction pipeline with LoRA support
â”‚
â”œâ”€â”€ research/                       # Jupyter notebooks for experimentation
â”‚   â”œâ”€â”€ 01_data_ingestion.ipynb
â”‚   â”œâ”€â”€ 02_data_validation.ipynb
â”‚   â”œâ”€â”€ 03_data_transformation.ipynb
â”‚   â”œâ”€â”€ 04_model_trainer.ipynb
â”‚   â””â”€â”€ 05_model_evaluation.ipynb
â”‚
â”œâ”€â”€ artifacts/                      # Generated during pipeline execution
â”‚   â”œâ”€â”€ data_ingestion/             # Downloaded SAMSum dataset
â”‚   â”œâ”€â”€ data_validation/            # Validation status
â”‚   â”œâ”€â”€ data_transformation/        # Tokenized datasets
â”‚   â”œâ”€â”€ model_trainer/              # LoRA adapters and tokenizer
â”‚   â””â”€â”€ model_evaluation/           # metrics.csv with ROUGE scores
â”‚
â””â”€â”€ tests/                          # Unit tests
    â””â”€â”€ unit/

## ğŸ”„ Development Workflows

Follow these steps when making changes to the pipeline:

1. **Update config.yaml** - Modify paths, model names, or data sources
2. **Update params.yaml** - Adjust hyperparameters (learning rate, epochs, LoRA rank, etc.)
3. **Update entity** - Define data classes for new configurations
4. **Update configuration manager** - Add config parsing in `src/text_summarizer/config/configuration.py`
5. **Update components** - Implement core logic in `src/text_summarizer/components/`
6. **Update pipeline** - Wire components in `src/text_summarizer/pipeline/`
7. **Update main.py** - Add new pipeline stages
8. **Update app.py** - Expose new endpoints if needed

## ğŸ› ï¸ Installation & Setup

### Prerequisites
- Python 3.10+
- Conda (recommended) or virtualenv
- Docker (for containerized deployment)
- AWS CLI (for ECS deployment)

### STEP 01: Clone the Repository

```bash
git clone https://github.com/yourusername/Text-Summarizer-Project.git
cd Text-Summarizer-Project
```

### STEP 02: Create Conda Environment

```bash
conda create -n textS python=3.10 -y
conda activate textS
```

### STEP 03: Install Dependencies

```bash
pip install -r requirements.txt
```

## ğŸƒ How to Run

### Option 1: Run Full Pipeline (Training + Evaluation)

```bash
python main.py
```

This executes all stages:
1. Data Ingestion
2. Data Validation
3. Data Transformation
4. Model Training (LoRA)
5. Model Evaluation

**Training Modes:**
- **Fast Mode** (1 hour): 100 samples, 1 epoch, LoRA rank 16
- **Quality Mode** (6-8 hours): ~80% dataset, 3 epochs, LoRA rank 16

Edit `src/text_summarizer/components/model_trainer.py` to switch modes.

### Option 2: Run FastAPI Application

```bash
# Development mode
python app.py
```

Or with Gunicorn for production:

```bash
gunicorn -k uvicorn.workers.UvicornWorker app:app --workers 2 --bind 0.0.0.0:8000
```

Access the API:
- **Docs**: http://localhost:8000/docs
- **Root**: http://localhost:8000/
- **Predict**: POST http://localhost:8000/predict

**Example cURL:**
```bash
curl -X POST http://localhost:8000/predict \
  -H "Content-Type: application/json" \
  -d '{"text": "Your long text here..."}'
```

### Stop the Application

Press `Ctrl+C` in the terminal, or:
```bash
pkill -f "python app.py"
pkill -f "gunicorn"
```

## ğŸ§  LoRA Migration & Challenges

### Migration from PEGASUS to LoRA

**Why LoRA?**
- **80-90% fewer trainable parameters** compared to full fine-tuning
- **Faster training**: 1 hour vs 6+ hours for similar quality
- **Lower memory footprint**: Trains on CPU/small GPUs
- **Modular adapters**: Easy to swap, version, and distribute

**Changes Made:**
1. Added `peft` to requirements
2. Updated `model_trainer.py`:
   - Applied `LoraConfig` with `target_modules=["q_proj", "v_proj"]`
   - Used `get_peft_model()` to wrap base PEGASUS
3. Updated `model_evaluation.py`:
   - Auto-detects LoRA adapters (`adapter_config.json`)
   - Falls back to standard model if no adapters found
4. Updated `prediction.py`:
   - Loads LoRA adapters via `PeftModel.from_pretrained()`
   - Falls back to base HuggingFace model if local artifacts missing

**See [LORA_MIGRATION.md](LORA_MIGRATION.md) for detailed migration notes.**

### Challenges Faced

| Challenge | Solution |
|-----------|----------|
| **Tokenizer path errors** | Used absolute paths; added fallback to base HuggingFace tokenizer |
| **Very low ROUGE scores** (0.01-0.05) | Increased LoRA rank from 8 to 16; used more training data and epochs |
| **Slow training** (6+ hours for 4% progress) | Created fast mode: 100 samples, 1 epoch, disabled eval during training |
| **Missing model artifacts on startup** | Implemented graceful fallback to download base model from HuggingFace |
| **Evaluation expecting LoRA adapters** | Added auto-detection logic to handle both LoRA and standard models |

## ğŸ“ˆ Model Performance

**Current Metrics** (Fast Mode - 1 hour training):
- ROUGE-1: ~0.15-0.25
- ROUGE-2: ~0.05-0.12
- ROUGE-L: ~0.12-0.20

**Quality Mode** (3 epochs, 80% dataset):
- Expected ROUGE-1: ~0.30-0.40
- Training Time: 6-8 hours on CPU

Check `artifacts/model_evaluation/metrics.csv` after training.

## ğŸ³ Docker & AWS Deployment

### Build Docker Image

```bash
docker build -t text-summarizer:latest .
```

### Run Locally

```bash
docker run -p 8000:8000 text-summarizer:latest
```

### Deploy to AWS ECS

See **[DEPLOY_ECS.md](DEPLOY_ECS.md)** for complete guide:

1. Create ECR repository
2. Push Docker image to ECR
3. Create ECS Task Definition (Fargate)
4. Create ECS Service with ALB
5. Test endpoints

**Quick ECR Push:**
```bash
# Replace with your details
aws ecr get-login-password --region eu-north-1 | \
  docker login --username AWS --password-stdin 474369734726.dkr.ecr.eu-north-1.amazonaws.com

docker tag text-summarizer:latest 474369734726.dkr.ecr.eu-north-1.amazonaws.com/text-s:latest
docker push 474369734726.dkr.ecr.eu-north-1.amazonaws.com/text-s:latest
```

## ğŸ” AWS Deployment Prerequisites

### IAM Permissions Required
- `AmazonEC2ContainerRegistryFullAccess`
- `AmazonEC2FullAccess`
- `AmazonECS_FullAccess`

### GitHub Secrets (for CI/CD)
```
AWS_ACCESS_KEY_ID=<your-key>
AWS_SECRET_ACCESS_KEY=<your-secret>
AWS_REGION=eu-north-1
AWS_ECR_LOGIN_URI=474369734726.dkr.ecr.eu-north-1.amazonaws.com
ECR_REPOSITORY_NAME=text-s
```

## ğŸ“š Additional Documentation

- **[FAST_TRAINING.md](FAST_TRAINING.md)** - Quick 1-hour training guide
- **[LORA_MIGRATION.md](LORA_MIGRATION.md)** - Detailed LoRA migration steps
- **[DEPLOY_ECS.md](DEPLOY_ECS.md)** - AWS ECS deployment walkthrough
- **[QUICKSTART.md](QUICKSTART.md)** - Quick start guide

## ğŸ§ª Testing

Run unit tests:
```bash
pytest tests/
```

## ğŸ“ Logs

Logs are saved to `logs/` directory. Each pipeline run creates a timestamped log file.

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

## ğŸ“§ Contact

**Author**: Sunil Verma  
**Project**: Text Summarizer with LoRA  
**Repository**: https://github.com/yourusername/Text-Summarizer-Project

## ğŸ“„ License

See [LICENSE](LICENSE) file for details.

---

**Built with â¤ï¸ using Transformers, PEFT, FastAPI, and AWS** 

